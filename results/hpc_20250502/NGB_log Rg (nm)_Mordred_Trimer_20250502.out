


OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 176), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013860445361298013), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05200262607672773), ('regressor__regressor__n_estimators', 385), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02774868567224034), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03233629747867359), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006913669486502273), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0994344812097677), ('regressor__regressor__n_estimators', 376), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0003662924116795776), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.019500953181626954), ('regressor__regressor__n_estimators', 594), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.034769668589043685), ('regressor__regressor__n_estimators', 910), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0005733412006678069), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 1311), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0010582620736238112), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 170), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.00022866373594870935), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05944038358217725), ('regressor__regressor__n_estimators', 1190), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.00016164219434149904), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03252108800594495), ('regressor__regressor__n_estimators', 1300), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.007988179462781242), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03279037873728727), ('regressor__regressor__n_estimators', 943), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.015778056533731495), ('regressor__regressor__n_estimators', 916), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013868717470737672), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.041835659184599416), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02564437149258721), ('regressor__regressor__n_estimators', 1055), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06319638640791223), ('regressor__regressor__n_estimators', 1992), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.00979042253960279), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05518505841179711), ('regressor__regressor__n_estimators', 249), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.008672088497302286), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0989922085581526), ('regressor__regressor__n_estimators', 1125), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.007312957455061442), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05913781436124809), ('regressor__regressor__n_estimators', 875), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.003374979707751342), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 74), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0072168417037699875), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09968645259195529), ('regressor__regressor__n_estimators', 803), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.008807110177308385), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0710591062751017), ('regressor__regressor__n_estimators', 158), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01908686927829978), ('regressor__regressor__n_estimators', 398), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.00011644471999936355), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.012488549132303144), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.061050709456446826), ('regressor__regressor__n_estimators', 638), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05580767018639971), ('regressor__regressor__n_estimators', 373), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.00010570022801177013), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02524641435759066), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.053865671998789805), ('regressor__regressor__n_estimators', 1991), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0002103372146762602), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04032833031280116), ('regressor__regressor__n_estimators', 965), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.008804907927301885), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09999133787326844), ('regressor__regressor__n_estimators', 1047), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.00013812071953130738), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005920341721764379), ('regressor__regressor__n_estimators', 1928), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.004835801458289747), ('regressor__regressor__verbose', False)])


Average scores:	 r: 0.84±0.07	 r2: 0.7±0.12
Filename: (Mordred-concentration-temperature-solvent dP-solvent dD-solvent dH)_NGB_Standard
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n02>
Subject: Job 646904: <NGB_log Rg (nm)_Mordred_Trimer_20250502> in cluster <Hazel> Done

Job <NGB_log Rg (nm)_Mordred_Trimer_20250502> was submitted from host <c205n12> by user <sdehgha2> in cluster <Hazel> at Fri May  2 09:47:45 2025
Job was executed on host(s) <6*c202n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri May  2 09:47:47 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri May  2 09:47:47 2025
Terminated at Fri May  2 19:53:19 2025
Results reported at Fri May  2 19:53:19 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input



#BSUB -n 6
#BSUB -W 40:05
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "NGB_log Rg (nm)_Mordred_Trimer_20250502"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250502/NGB_log Rg (nm)_Mordred_Trimer_20250502.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250502/NGB_log Rg (nm)_Mordred__Trimer_20250502.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py --target_features "log Rg (nm)"                                       --representation "Mordred"                                       --regressor_type "NGB"                                       --oligomer_representation "Trimer"                                       --numerical_feats 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' 'solvent dP' 'solvent dD' 'solvent dH' 




------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   164299.11 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.99 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               13.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   36343 sec.
    Turnaround time :                            36334 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250502/NGB_log Rg (nm)_Mordred__Trimer_20250502.err> for stderr output of this job.

