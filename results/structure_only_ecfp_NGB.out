RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1522), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1722), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009786891218909548), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1983), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009810245279260111), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1959), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1286), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1764), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1422), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008212093846171939), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000979822883652993), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1996), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1210), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009756647277722969), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1990), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1762), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1665), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007874931125061603), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1775), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1657), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007271778889344074), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006951773418639219), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1750), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009010687533690505), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1717), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006626918518403176), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1721), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1724), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1795), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000847337439581784), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1774), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1341), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008453487561980327), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1459), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


Average scores:	 r: 0.42±0.19	 r2: 0.16±0.18
RRU Dimer
Filename: (ECFP3.binary.512)_NGB
Saved results to:
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.binary.512)_NGB_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.binary.512)_NGB_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.binary.512)_NGB_predictions.csv

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n06>
Subject: Job 844813: <ecfp_radius_tructure_only> in cluster <Hazel> Done

Job <ecfp_radius_tructure_only> was submitted from host <c028n03> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 12:04:13 2024
Job was executed on host(s) <4*c201n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 12:04:15 2024
                            <4*c201n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training> was used as the working directory.
Started at Mon Oct 21 12:04:15 2024
Terminated at Mon Oct 21 18:38:25 2024
Results reported at Mon Oct 21 18:38:25 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   105333.39 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.86 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   23656 sec.
    Turnaround time :                            23652 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n01>
Subject: Job 848784: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:25 2024
Job was executed on host(s) <4*c207n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
                            <4*c207n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:05:26 2024
Terminated at Mon Oct 21 19:05:49 2024
Results reported at Mon Oct 21 19:05:49 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.17 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   32 sec.
    Turnaround time :                            24 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n09>
Subject: Job 848783: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:25 2024
Job was executed on host(s) <4*c205n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
                            <4*c205n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:05:26 2024
Terminated at Mon Oct 21 19:05:49 2024
Results reported at Mon Oct 21 19:05:49 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   32.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   42 sec.
    Turnaround time :                            24 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n11>
Subject: Job 848786: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:25 2024
Job was executed on host(s) <4*c207n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:51 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:05:51 2024
Terminated at Mon Oct 21 19:06:17 2024
Results reported at Mon Oct 21 19:06:17 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.13 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   26 sec.
    Turnaround time :                            52 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n09>
Subject: Job 848788: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c203n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:51 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:05:51 2024
Terminated at Mon Oct 21 19:06:18 2024
Results reported at Mon Oct 21 19:06:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.51 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   35 sec.
    Turnaround time :                            52 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n06>
Subject: Job 848785: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:25 2024
Job was executed on host(s) <4*c205n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:51 2024
                            <4*c205n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:05:51 2024
Terminated at Mon Oct 21 19:06:20 2024
Results reported at Mon Oct 21 19:06:20 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   39 sec.
    Turnaround time :                            55 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n03>
Subject: Job 848789: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c203n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:51 2024
                            <4*c203n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:05:51 2024
Terminated at Mon Oct 21 19:06:20 2024
Results reported at Mon Oct 21 19:06:20 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.12 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   46 sec.
    Turnaround time :                            54 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n05>
Subject: Job 848787: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c203n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:51 2024
                            <4*c203n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:05:51 2024
Terminated at Mon Oct 21 19:06:21 2024
Results reported at Mon Oct 21 19:06:21 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.56 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   33 sec.
    Turnaround time :                            55 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n14>
Subject: Job 848790: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c205n14>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:06:25 2024
                            <4*c205n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:06:25 2024
Terminated at Mon Oct 21 19:06:50 2024
Results reported at Mon Oct 21 19:06:50 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   30.11 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   49 sec.
    Turnaround time :                            84 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n09>
Subject: Job 848791: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c205n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:06:27 2024
                            <4*c205n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:06:27 2024
Terminated at Mon Oct 21 19:06:50 2024
Results reported at Mon Oct 21 19:06:50 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   31.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   30 sec.
    Turnaround time :                            84 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n01>
Subject: Job 848792: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c207n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:06:27 2024
                            <4*c207n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:06:27 2024
Terminated at Mon Oct 21 19:06:51 2024
Results reported at Mon Oct 21 19:06:51 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.41 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   29 sec.
    Turnaround time :                            85 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n03>
Subject: Job 848797: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c203n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:06:53 2024
                            <4*c203n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:06:53 2024
Terminated at Mon Oct 21 19:07:18 2024
Results reported at Mon Oct 21 19:07:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.37 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   27 sec.
    Turnaround time :                            112 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n09>
Subject: Job 848796: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c203n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:06:53 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:06:53 2024
Terminated at Mon Oct 21 19:07:19 2024
Results reported at Mon Oct 21 19:07:19 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.09 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   31 sec.
    Turnaround time :                            113 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n11>
Subject: Job 848794: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c207n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:06:53 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:06:53 2024
Terminated at Mon Oct 21 19:07:20 2024
Results reported at Mon Oct 21 19:07:20 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.50 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   33 sec.
    Turnaround time :                            114 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n05>
Subject: Job 848795: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c203n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:06:53 2024
                            <4*c203n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:06:53 2024
Terminated at Mon Oct 21 19:07:22 2024
Results reported at Mon Oct 21 19:07:22 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.52 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   31 sec.
    Turnaround time :                            116 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n06>
Subject: Job 848793: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c205n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:06:53 2024
                            <4*c205n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:06:53 2024
Terminated at Mon Oct 21 19:07:23 2024
Results reported at Mon Oct 21 19:07:23 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   33 sec.
    Turnaround time :                            117 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n09>
Subject: Job 848799: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c205n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:07:26 2024
                            <4*c205n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:07:26 2024
Terminated at Mon Oct 21 19:07:49 2024
Results reported at Mon Oct 21 19:07:49 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   32.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   29 sec.
    Turnaround time :                            143 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n01>
Subject: Job 848800: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c207n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:07:26 2024
                            <4*c207n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:07:26 2024
Terminated at Mon Oct 21 19:07:49 2024
Results reported at Mon Oct 21 19:07:49 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.10 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   25 sec.
    Turnaround time :                            143 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n13>
Subject: Job 848798: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c205n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:07:26 2024
                            <4*c205n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:07:26 2024
Terminated at Mon Oct 21 19:07:50 2024
Results reported at Mon Oct 21 19:07:50 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   31.04 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   30 sec.
    Turnaround time :                            144 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n09>
Subject: Job 848804: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c203n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:07:52 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:07:52 2024
Terminated at Mon Oct 21 19:08:18 2024
Results reported at Mon Oct 21 19:08:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.18 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   29 sec.
    Turnaround time :                            171 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n11>
Subject: Job 848802: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c207n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:07:52 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:07:52 2024
Terminated at Mon Oct 21 19:08:18 2024
Results reported at Mon Oct 21 19:08:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   28 sec.
    Turnaround time :                            172 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n03>
Subject: Job 848805: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c203n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:07:52 2024
                            <4*c203n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:07:52 2024
Terminated at Mon Oct 21 19:08:20 2024
Results reported at Mon Oct 21 19:08:20 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.59 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   29 sec.
    Turnaround time :                            173 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n05>
Subject: Job 848803: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c203n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:07:52 2024
                            <4*c203n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:07:52 2024
Terminated at Mon Oct 21 19:08:21 2024
Results reported at Mon Oct 21 19:08:21 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.21 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   59 sec.
    Turnaround time :                            174 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n06>
Subject: Job 848801: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:26 2024
Job was executed on host(s) <4*c205n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:07:52 2024
                            <4*c205n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:07:52 2024
Terminated at Mon Oct 21 19:08:22 2024
Results reported at Mon Oct 21 19:08:22 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26.15 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   56 sec.
    Turnaround time :                            176 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n14>
Subject: Job 848806: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c205n14>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:08:26 2024
                            <4*c205n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:08:26 2024
Terminated at Mon Oct 21 19:08:54 2024
Results reported at Mon Oct 21 19:08:54 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   33.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   34 sec.
    Turnaround time :                            207 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n01>
Subject: Job 848808: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c207n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:08:26 2024
                            <4*c207n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:08:26 2024
Terminated at Mon Oct 21 19:08:54 2024
Results reported at Mon Oct 21 19:08:54 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.58 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   34 sec.
    Turnaround time :                            207 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n09>
Subject: Job 848807: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c205n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:08:26 2024
                            <4*c205n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:08:26 2024
Terminated at Mon Oct 21 19:08:54 2024
Results reported at Mon Oct 21 19:08:54 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   34.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   35 sec.
    Turnaround time :                            207 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n11>
Subject: Job 848810: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c207n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:08:56 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:08:56 2024
Terminated at Mon Oct 21 19:09:27 2024
Results reported at Mon Oct 21 19:09:27 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.27 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   39 sec.
    Turnaround time :                            240 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n09>
Subject: Job 848812: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c203n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:08:56 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:08:56 2024
Terminated at Mon Oct 21 19:09:27 2024
Results reported at Mon Oct 21 19:09:27 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.60 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   39 sec.
    Turnaround time :                            240 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n03>
Subject: Job 848813: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c203n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:08:56 2024
                            <4*c203n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:08:56 2024
Terminated at Mon Oct 21 19:09:27 2024
Results reported at Mon Oct 21 19:09:27 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.23 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   37 sec.
    Turnaround time :                            240 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n05>
Subject: Job 848811: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c203n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:08:56 2024
                            <4*c203n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:08:56 2024
Terminated at Mon Oct 21 19:09:30 2024
Results reported at Mon Oct 21 19:09:30 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.51 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   39 sec.
    Turnaround time :                            243 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n06>
Subject: Job 848809: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c205n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:08:56 2024
                            <4*c205n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:08:56 2024
Terminated at Mon Oct 21 19:09:30 2024
Results reported at Mon Oct 21 19:09:30 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26.07 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   35 sec.
    Turnaround time :                            243 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n01>
Subject: Job 848816: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c207n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:09:27 2024
                            <4*c207n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:09:27 2024
Terminated at Mon Oct 21 19:09:57 2024
Results reported at Mon Oct 21 19:09:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.47 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   60 sec.
    Turnaround time :                            270 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n09>
Subject: Job 848815: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c205n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:09:27 2024
                            <4*c205n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:09:27 2024
Terminated at Mon Oct 21 19:09:57 2024
Results reported at Mon Oct 21 19:09:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   34.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   60 sec.
    Turnaround time :                            270 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n13>
Subject: Job 848814: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c205n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:09:27 2024
                            <4*c205n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:09:27 2024
Terminated at Mon Oct 21 19:09:57 2024
Results reported at Mon Oct 21 19:09:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   31.38 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   33 sec.
    Turnaround time :                            270 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n11>
Subject: Job 848818: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c207n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:09:59 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:09:59 2024
Terminated at Mon Oct 21 19:10:32 2024
Results reported at Mon Oct 21 19:10:32 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   62 sec.
    Turnaround time :                            305 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n13>
Subject: Job 848820: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c203n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:09:59 2024
                            <4*c203n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:09:59 2024
Terminated at Mon Oct 21 19:10:34 2024
Results reported at Mon Oct 21 19:10:34 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   37 sec.
    Turnaround time :                            307 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n03>
Subject: Job 848821: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:28 2024
Job was executed on host(s) <4*c203n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:09:59 2024
                            <4*c203n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:09:59 2024
Terminated at Mon Oct 21 19:10:35 2024
Results reported at Mon Oct 21 19:10:35 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.18 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.25 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              13
    Max Threads :                                15
    Run time :                                   38 sec.
    Turnaround time :                            307 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n05>
Subject: Job 848819: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c203n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:09:59 2024
                            <4*c203n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:09:59 2024
Terminated at Mon Oct 21 19:10:35 2024
Results reported at Mon Oct 21 19:10:35 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.60 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.25 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              21
    Max Threads :                                23
    Run time :                                   62 sec.
    Turnaround time :                            308 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n06>
Subject: Job 848817: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:27 2024
Job was executed on host(s) <4*c205n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:09:59 2024
                            <4*c205n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:09:59 2024
Terminated at Mon Oct 21 19:10:36 2024
Results reported at Mon Oct 21 19:10:36 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26.25 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.25 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              28
    Max Threads :                                30
    Run time :                                   63 sec.
    Turnaround time :                            309 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n06>
Subject: Job 848822: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:28 2024
Job was executed on host(s) <4*c207n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:10:17 2024
                            <4*c207n07>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:10:17 2024
Terminated at Mon Oct 21 19:10:45 2024
Results reported at Mon Oct 21 19:10:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25.01 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   28 sec.
    Turnaround time :                            317 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n01>
Subject: Job 848825: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:28 2024
Job was executed on host(s) <4*c207n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:10:26 2024
                            <4*c207n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:10:26 2024
Terminated at Mon Oct 21 19:10:57 2024
Results reported at Mon Oct 21 19:10:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.59 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   56 sec.
    Turnaround time :                            329 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n09>
Subject: Job 848824: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:28 2024
Job was executed on host(s) <4*c205n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:10:26 2024
                            <4*c205n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:10:26 2024
Terminated at Mon Oct 21 19:10:57 2024
Results reported at Mon Oct 21 19:10:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   33.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   60 sec.
    Turnaround time :                            329 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n14>
Subject: Job 848823: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:28 2024
Job was executed on host(s) <4*c205n14>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:10:26 2024
                            <4*c205n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:10:26 2024
Terminated at Mon Oct 21 19:10:58 2024
Results reported at Mon Oct 21 19:10:58 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   31.12 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   61 sec.
    Turnaround time :                            330 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n11>
Subject: Job 848830: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:28 2024
Job was executed on host(s) <4*c207n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:10:58 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:10:58 2024
Terminated at Mon Oct 21 19:11:29 2024
Results reported at Mon Oct 21 19:11:29 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.49 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   57 sec.
    Turnaround time :                            361 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n06>
Subject: Job 848829: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:28 2024
Job was executed on host(s) <4*c205n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:10:58 2024
                            <4*c205n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:10:58 2024
Terminated at Mon Oct 21 19:11:30 2024
Results reported at Mon Oct 21 19:11:30 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26.26 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   51 sec.
    Turnaround time :                            362 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n04>
Subject: Job 848826: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:28 2024
Job was executed on host(s) <4*c202n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:10:58 2024
                            <4*c202n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:10:58 2024
Terminated at Mon Oct 21 19:11:33 2024
Results reported at Mon Oct 21 19:11:33 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   32.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   51 sec.
    Turnaround time :                            365 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n07>
Subject: Job 848827: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:28 2024
Job was executed on host(s) <4*c202n07>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:10:58 2024
                            <4*c202n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:10:58 2024
Terminated at Mon Oct 21 19:11:37 2024
Results reported at Mon Oct 21 19:11:37 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   31.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.25 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   54 sec.
    Turnaround time :                            369 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n03>
Subject: Job 848828: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:05:28 2024
Job was executed on host(s) <4*c202n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Oct 21 19:10:58 2024
                            <4*c202n08>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Mon Oct 21 19:10:58 2024
Terminated at Mon Oct 21 19:11:37 2024
Results reported at Mon Oct 21 19:11:37 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   35.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.25 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   55 sec.
    Turnaround time :                            369 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n09>
Subject: Job 849556: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n05> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:21:03 2024
Job was executed on host(s) <4*c202n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:21:03 2024
                            <4*c202n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:21:03 2024
Terminated at Tue Oct 22 10:23:59 2024
Results reported at Tue Oct 22 10:23:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   4.91 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   202 sec.
    Turnaround time :                            176 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n10>
Subject: Job 849557: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n05> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:21:04 2024
Job was executed on host(s) <4*c205n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:21:04 2024
                            <4*c205n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:21:04 2024
Terminated at Tue Oct 22 10:23:59 2024
Results reported at Tue Oct 22 10:23:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   4.48 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   194 sec.
    Turnaround time :                            175 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 849559: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n05> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:21:04 2024
Job was executed on host(s) <4*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:21:05 2024
                            <4*c200n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:21:05 2024
Terminated at Tue Oct 22 10:23:59 2024
Results reported at Tue Oct 22 10:23:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   9.53 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.25 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   197 sec.
    Turnaround time :                            175 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n06>
Subject: Job 849560: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n05> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:21:05 2024
Job was executed on host(s) <4*c201n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:22:02 2024
                            <4*c201n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:22:02 2024
Terminated at Tue Oct 22 10:23:59 2024
Results reported at Tue Oct 22 10:23:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   3.80 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   129 sec.
    Turnaround time :                            174 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n12>
Subject: Job 849587: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n05> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:21:15 2024
Job was executed on host(s) <4*c200n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:24:00 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:24:00 2024
Terminated at Tue Oct 22 10:24:00 2024
Results reported at Tue Oct 22 10:24:00 2024
Cannot open your job file: /home/sdehgha2/.lsbatch/1729606875.849587
TERM_OWNER: job killed by owner.
Exited with signal termination: 2.

Resource usage summary:

    CPU time :                                   0.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              1
    Max Threads :                                1
    Run time :                                   1 sec.
    Turnaround time :                            165 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n14>
Subject: Job 849558: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n05> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:21:04 2024
Job was executed on host(s) <4*c207n14>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:21:05 2024
                            <4*c207n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:21:05 2024
Terminated at Tue Oct 22 10:24:16 2024
Results reported at Tue Oct 22 10:24:16 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   6.11 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   209 sec.
    Turnaround time :                            192 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n06>
Subject: Job 849647: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c201n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:15 2024
                            <4*c201n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:42:15 2024
Terminated at Tue Oct 22 10:42:35 2024
Results reported at Tue Oct 22 10:42:35 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26.10 sec.
    Max Memory :                                 1 GB
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              19
    Max Threads :                                21
    Run time :                                   29 sec.
    Turnaround time :                            34 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n06>
Subject: Job 849648: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c201n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:43:15 2024
                            <4*c201n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:43:15 2024
Terminated at Tue Oct 22 10:43:34 2024
Results reported at Tue Oct 22 10:43:34 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   29 sec.
    Turnaround time :                            93 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n06>
Subject: Job 849649: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c201n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:44:16 2024
                            <4*c201n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:44:16 2024
Terminated at Tue Oct 22 10:44:37 2024
Results reported at Tue Oct 22 10:44:37 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   28.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   33 sec.
    Turnaround time :                            156 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n12>
Subject: Job 849650: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c200n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:44:35 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:44:35 2024
Terminated at Tue Oct 22 10:44:54 2024
Results reported at Tue Oct 22 10:44:54 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   20 sec.
    Turnaround time :                            173 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n05>
Subject: Job 849651: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c201n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:45:15 2024
                            <4*c201n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:45:15 2024
Terminated at Tue Oct 22 10:45:35 2024
Results reported at Tue Oct 22 10:45:35 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   30.10 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   34 sec.
    Turnaround time :                            214 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 849652: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:45:35 2024
                            <4*c200n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:45:35 2024
Terminated at Tue Oct 22 10:45:52 2024
Results reported at Tue Oct 22 10:45:52 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.17 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   28 sec.
    Turnaround time :                            231 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n05>
Subject: Job 849653: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c201n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:46:15 2024
                            <4*c201n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:46:15 2024
Terminated at Tue Oct 22 10:46:37 2024
Results reported at Tue Oct 22 10:46:37 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   31.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   29 sec.
    Turnaround time :                            276 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n12>
Subject: Job 849654: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c200n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:46:36 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:46:36 2024
Terminated at Tue Oct 22 10:46:54 2024
Results reported at Tue Oct 22 10:46:54 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.17 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   32 sec.
    Turnaround time :                            293 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n05>
Subject: Job 849655: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c201n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:47:15 2024
                            <4*c201n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:47:15 2024
Terminated at Tue Oct 22 10:47:36 2024
Results reported at Tue Oct 22 10:47:36 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   31.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   29 sec.
    Turnaround time :                            335 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 849656: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:47:36 2024
                            <4*c200n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:47:36 2024
Terminated at Tue Oct 22 10:47:56 2024
Results reported at Tue Oct 22 10:47:56 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.18 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   32 sec.
    Turnaround time :                            355 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c010n04>
Subject: Job 849657: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c010n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:47:40 2024
                            <4*c005n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:47:40 2024
Terminated at Tue Oct 22 10:47:59 2024
Results reported at Tue Oct 22 10:47:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.29 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   20 sec.
    Turnaround time :                            358 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n06>
Subject: Job 849658: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c201n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:48:15 2024
                            <4*c201n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:48:15 2024
Terminated at Tue Oct 22 10:48:37 2024
Results reported at Tue Oct 22 10:48:37 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   28.30 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   31 sec.
    Turnaround time :                            396 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 849659: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:48:35 2024
                            <4*c200n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:48:35 2024
Terminated at Tue Oct 22 10:48:55 2024
Results reported at Tue Oct 22 10:48:55 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   29 sec.
    Turnaround time :                            414 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c005n03>
Subject: Job 849660: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c005n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:48:41 2024
                            <4*c010n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:48:41 2024
Terminated at Tue Oct 22 10:48:58 2024
Results reported at Tue Oct 22 10:48:58 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.08 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   28 sec.
    Turnaround time :                            417 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n06>
Subject: Job 849661: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c201n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:49:15 2024
                            <4*c201n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:49:15 2024
Terminated at Tue Oct 22 10:49:35 2024
Results reported at Tue Oct 22 10:49:35 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27.24 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   25 sec.
    Turnaround time :                            454 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 849662: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:49:35 2024
                            <4*c200n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:49:35 2024
Terminated at Tue Oct 22 10:49:56 2024
Results reported at Tue Oct 22 10:49:56 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.05 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   28 sec.
    Turnaround time :                            475 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c005n03>
Subject: Job 849663: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:01 2024
Job was executed on host(s) <4*c005n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:49:40 2024
                            <4*c010n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:49:40 2024
Terminated at Tue Oct 22 10:49:59 2024
Results reported at Tue Oct 22 10:49:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.31 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   31 sec.
    Turnaround time :                            478 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n06>
Subject: Job 849664: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c201n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:50:15 2024
                            <4*c201n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:50:15 2024
Terminated at Tue Oct 22 10:50:34 2024
Results reported at Tue Oct 22 10:50:34 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27.48 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   26 sec.
    Turnaround time :                            512 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n12>
Subject: Job 849665: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c200n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:50:36 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:50:36 2024
Terminated at Tue Oct 22 10:50:56 2024
Results reported at Tue Oct 22 10:50:56 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.09 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   30 sec.
    Turnaround time :                            534 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c005n03>
Subject: Job 849666: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c005n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:50:41 2024
                            <4*c010n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:50:41 2024
Terminated at Tue Oct 22 10:50:56 2024
Results reported at Tue Oct 22 10:50:56 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   24 sec.
    Turnaround time :                            534 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n06>
Subject: Job 849667: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c201n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:51:15 2024
                            <4*c201n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:51:15 2024
Terminated at Tue Oct 22 10:51:35 2024
Results reported at Tue Oct 22 10:51:35 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27.54 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   31 sec.
    Turnaround time :                            573 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n05>
Subject: Job 849668: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c207n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:51:29 2024
                            <4*c207n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:51:29 2024
Terminated at Tue Oct 22 10:51:52 2024
Results reported at Tue Oct 22 10:51:52 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.37 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   35 sec.
    Turnaround time :                            590 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n12>
Subject: Job 849669: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c200n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:51:37 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:51:37 2024
Terminated at Tue Oct 22 10:51:57 2024
Results reported at Tue Oct 22 10:51:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.04 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   31 sec.
    Turnaround time :                            595 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c005n03>
Subject: Job 849670: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c005n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:51:40 2024
                            <4*c010n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:51:40 2024
Terminated at Tue Oct 22 10:51:58 2024
Results reported at Tue Oct 22 10:51:58 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.01 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   32 sec.
    Turnaround time :                            596 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 849671: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:52:05 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:52:05 2024
Terminated at Tue Oct 22 10:52:29 2024
Results reported at Tue Oct 22 10:52:29 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27.55 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   26 sec.
    Turnaround time :                            627 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n06>
Subject: Job 849672: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c201n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:52:15 2024
                            <4*c201n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:52:15 2024
Terminated at Tue Oct 22 10:52:36 2024
Results reported at Tue Oct 22 10:52:36 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27.38 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   28 sec.
    Turnaround time :                            634 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n05>
Subject: Job 849673: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c207n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:52:30 2024
                            <4*c207n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:52:30 2024
Terminated at Tue Oct 22 10:52:53 2024
Results reported at Tue Oct 22 10:52:53 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.10 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   31 sec.
    Turnaround time :                            651 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 849674: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:52:36 2024
                            <4*c200n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:52:36 2024
Terminated at Tue Oct 22 10:52:57 2024
Results reported at Tue Oct 22 10:52:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   30 sec.
    Turnaround time :                            655 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c005n03>
Subject: Job 849675: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c005n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:52:40 2024
                            <4*c010n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:52:40 2024
Terminated at Tue Oct 22 10:52:58 2024
Results reported at Tue Oct 22 10:52:58 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.03 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   30 sec.
    Turnaround time :                            656 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 849676: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:53:04 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:53:04 2024
Terminated at Tue Oct 22 10:53:25 2024
Results reported at Tue Oct 22 10:53:25 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26.56 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   26 sec.
    Turnaround time :                            683 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n06>
Subject: Job 849677: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c201n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:53:15 2024
                            <4*c201n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:53:15 2024
Terminated at Tue Oct 22 10:53:35 2024
Results reported at Tue Oct 22 10:53:35 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   26 sec.
    Turnaround time :                            693 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n05>
Subject: Job 849678: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c207n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:53:29 2024
                            <4*c207n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:53:29 2024
Terminated at Tue Oct 22 10:53:53 2024
Results reported at Tue Oct 22 10:53:53 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.25 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   30 sec.
    Turnaround time :                            711 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n12>
Subject: Job 849679: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c200n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:53:36 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:53:36 2024
Terminated at Tue Oct 22 10:54:00 2024
Results reported at Tue Oct 22 10:54:00 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.16 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   33 sec.
    Turnaround time :                            718 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c005n03>
Subject: Job 849680: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c005n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:53:40 2024
                            <4*c010n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:53:40 2024
Terminated at Tue Oct 22 10:54:04 2024
Results reported at Tue Oct 22 10:54:04 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.35 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   36 sec.
    Turnaround time :                            722 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 849681: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:02 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:54:04 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:54:04 2024
Terminated at Tue Oct 22 10:54:27 2024
Results reported at Tue Oct 22 10:54:27 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27.33 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   32 sec.
    Turnaround time :                            745 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n05>
Subject: Job 849682: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c201n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:54:16 2024
                            <4*c201n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:54:16 2024
Terminated at Tue Oct 22 10:54:43 2024
Results reported at Tue Oct 22 10:54:43 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   31.31 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   34 sec.
    Turnaround time :                            760 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n05>
Subject: Job 849683: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c207n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:54:29 2024
                            <4*c207n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:54:29 2024
Terminated at Tue Oct 22 10:54:52 2024
Results reported at Tue Oct 22 10:54:52 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   29 sec.
    Turnaround time :                            769 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 849684: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:54:36 2024
                            <4*c200n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:54:36 2024
Terminated at Tue Oct 22 10:54:57 2024
Results reported at Tue Oct 22 10:54:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.59 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   27 sec.
    Turnaround time :                            774 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c005n03>
Subject: Job 849685: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c005n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:54:41 2024
                            <4*c010n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:54:41 2024
Terminated at Tue Oct 22 10:54:58 2024
Results reported at Tue Oct 22 10:54:58 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.09 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   24 sec.
    Turnaround time :                            775 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 849686: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:55:04 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:55:04 2024
Terminated at Tue Oct 22 10:55:24 2024
Results reported at Tue Oct 22 10:55:24 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26.26 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   24 sec.
    Turnaround time :                            801 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n05>
Subject: Job 849687: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c201n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:55:16 2024
                            <4*c201n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:55:16 2024
Terminated at Tue Oct 22 10:55:38 2024
Results reported at Tue Oct 22 10:55:38 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   30.31 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   52 sec.
    Turnaround time :                            815 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n05>
Subject: Job 849688: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c207n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:55:29 2024
                            <4*c207n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:55:29 2024
Terminated at Tue Oct 22 10:55:53 2024
Results reported at Tue Oct 22 10:55:53 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.68 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   28 sec.
    Turnaround time :                            830 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n12>
Subject: Job 849689: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c200n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:55:37 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:55:37 2024
Terminated at Tue Oct 22 10:56:00 2024
Results reported at Tue Oct 22 10:56:00 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.34 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   33 sec.
    Turnaround time :                            837 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c005n03>
Subject: Job 849690: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c005n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:55:40 2024
                            <4*c010n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:55:40 2024
Terminated at Tue Oct 22 10:56:03 2024
Results reported at Tue Oct 22 10:56:03 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   23 sec.
    Turnaround time :                            840 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n09>
Subject: Job 849691: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c202n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:55:49 2024
                            <4*c202n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:55:49 2024
Terminated at Tue Oct 22 10:56:10 2024
Results reported at Tue Oct 22 10:56:10 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   28.47 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   5 sec.
    Turnaround time :                            847 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 849692: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:56:04 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:56:04 2024
Terminated at Tue Oct 22 10:56:26 2024
Results reported at Tue Oct 22 10:56:26 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   32 sec.
    Turnaround time :                            864 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n05>
Subject: Job 849693: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c201n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:56:15 2024
                            <4*c201n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:56:15 2024
Terminated at Tue Oct 22 10:56:39 2024
Results reported at Tue Oct 22 10:56:39 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   33.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   31 sec.
    Turnaround time :                            876 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR RF 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__max_depth', None), ('regressor__regressor__max_features', 'log2'), ('regressor__regressor__min_samples_leaf', 0.9268959189169639), ('regressor__regressor__min_samples_split', 0.34685161787782576), ('regressor__regressor__n_estimators', 592)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 520)}
Average scores:	 r: nan±nan	 r2: -0.0±0.0
Monomer
Filename: (Mordred)_RF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(Mordred)_RF_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n05>
Subject: Job 849694: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c201n06> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:42:03 2024
Job was executed on host(s) <4*c207n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 10:56:29 2024
                            <4*c207n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 10:56:29 2024
Terminated at Tue Oct 22 10:56:49 2024
Results reported at Tue Oct 22 10:56:49 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.67 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   26 sec.
    Turnaround time :                            886 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.36±0.0	 r2: 0.01±0.0
Dimer
Filename: (ECFP3.binary.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Dimer/(ECFP3.binary.512)_NGB_scores.json


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.35±0.01	 r2: 0.01±0.0
RRU Dimer
Filename: (ECFP3.binary.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Dimer/(ECFP3.binary.512)_NGB_scores.json


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.35±0.0	 r2: 0.01±0.0
Monomer
Filename: (ECFP3.binary.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(ECFP3.binary.512)_NGB_scores.json


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.34±0.0	 r2: 0.01±0.0
RRU Monomer
Filename: (ECFP3.binary.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Monomer/(ECFP3.binary.512)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n01>
Subject: Job 850225: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:48 2024
Job was executed on host(s) <4*c207n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
                            <4*c207n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:12:49 2024
Terminated at Tue Oct 22 11:14:59 2024
Results reported at Tue Oct 22 11:14:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   180.21 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.43 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   145 sec.
    Turnaround time :                            131 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.36±0.0	 r2: 0.01±0.0
Trimer
Filename: (ECFP3.binary.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Trimer/(ECFP3.binary.512)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n12>
Subject: Job 850228: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:48 2024
Job was executed on host(s) <4*c200n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:12:49 2024
Terminated at Tue Oct 22 11:15:01 2024
Results reported at Tue Oct 22 11:15:01 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   186.34 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   147 sec.
    Turnaround time :                            133 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n09>
Subject: Job 850227: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:48 2024
Job was executed on host(s) <4*c207n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:12:49 2024
Terminated at Tue Oct 22 11:15:02 2024
Results reported at Tue Oct 22 11:15:02 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   200.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   142 sec.
    Turnaround time :                            134 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 850224: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:48 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:12:49 2024
Terminated at Tue Oct 22 11:15:02 2024
Results reported at Tue Oct 22 11:15:02 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   195.60 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   152 sec.
    Turnaround time :                            134 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n08>
Subject: Job 850226: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:48 2024
Job was executed on host(s) <4*c207n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
                            <4*c207n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:12:49 2024
Terminated at Tue Oct 22 11:15:03 2024
Results reported at Tue Oct 22 11:15:03 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   196.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   148 sec.
    Turnaround time :                            135 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.35±0.0	 r2: 0.01±0.0
RRU Trimer
Filename: (ECFP3.binary.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Trimer/(ECFP3.binary.512)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n10>
Subject: Job 850229: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:48 2024
Job was executed on host(s) <4*c202n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:13:48 2024
                            <4*c202n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:13:48 2024
Terminated at Tue Oct 22 11:16:00 2024
Results reported at Tue Oct 22 11:16:00 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   205.34 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   158 sec.
    Turnaround time :                            192 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.37±0.0	 r2: 0.01±0.0
Trimer
Filename: (ECFP3.count.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Trimer/(ECFP3.count.512)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n11>
Subject: Job 850232: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c207n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:15:03 2024
                            <4*c207n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:15:03 2024
Terminated at Tue Oct 22 11:17:01 2024
Results reported at Tue Oct 22 11:17:01 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   177.13 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.50 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   118 sec.
    Turnaround time :                            252 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.37±0.0	 r2: 0.01±0.0
RRU Monomer
Filename: (ECFP3.count.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Monomer/(ECFP3.count.512)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n08>
Subject: Job 850233: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c207n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:15:03 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:15:03 2024
Terminated at Tue Oct 22 11:17:09 2024
Results reported at Tue Oct 22 11:17:09 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   192.08 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   126 sec.
    Turnaround time :                            260 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.37±0.0	 r2: 0.01±0.0
Dimer
Filename: (ECFP3.count.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Dimer/(ECFP3.count.512)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 850231: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:15:03 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:15:03 2024
Terminated at Tue Oct 22 11:17:14 2024
Results reported at Tue Oct 22 11:17:14 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   203.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   132 sec.
    Turnaround time :                            265 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.37±0.0	 r2: 0.01±0.0
Monomer
Filename: (ECFP3.count.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(ECFP3.count.512)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 850230: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:15:01 2024
                            <4*c200n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:15:01 2024
Terminated at Tue Oct 22 11:17:19 2024
Results reported at Tue Oct 22 11:17:19 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   214.18 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   138 sec.
    Turnaround time :                            270 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.37±0.0	 r2: 0.01±0.0
RRU Trimer
Filename: (ECFP3.count.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Trimer/(ECFP3.count.512)_NGB_scores.json


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.36±0.0	 r2: 0.01±0.0
RRU Dimer
Filename: (ECFP3.count.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Dimer/(ECFP3.count.512)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n01>
Subject: Job 850235: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c207n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:16:00 2024
                            <4*c207n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:16:00 2024
Terminated at Tue Oct 22 11:18:10 2024
Results reported at Tue Oct 22 11:18:10 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   193.43 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   131 sec.
    Turnaround time :                            321 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n09>
Subject: Job 850234: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c202n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:16:00 2024
                            <4*c202n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:16:00 2024
Terminated at Tue Oct 22 11:18:13 2024
Results reported at Tue Oct 22 11:18:13 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   207.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   133 sec.
    Turnaround time :                            324 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.37±0.0	 r2: 0.01±0.0
Dimer
Filename: (ECFP4.binary.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Dimer/(ECFP4.binary.1024)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n09>
Subject: Job 850237: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c207n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:17:16 2024
                            <4*c207n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:17:16 2024
Terminated at Tue Oct 22 11:19:21 2024
Results reported at Tue Oct 22 11:19:21 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   194.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.50 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   140 sec.
    Turnaround time :                            392 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.36±0.0	 r2: 0.01±0.0
Monomer
Filename: (ECFP4.binary.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(ECFP4.binary.1024)_NGB_scores.json


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.35±0.0	 r2: 0.01±0.0
RRU Monomer
Filename: (ECFP4.binary.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Monomer/(ECFP4.binary.1024)_NGB_scores.json


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.37±0.0	 r2: 0.01±0.0
Trimer
Filename: (ECFP4.binary.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Trimer/(ECFP4.binary.1024)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 850236: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:17:16 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:17:16 2024
Terminated at Tue Oct 22 11:19:34 2024
Results reported at Tue Oct 22 11:19:34 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   214.41 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   140 sec.
    Turnaround time :                            405 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n12>
Subject: Job 850239: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c200n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:17:19 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:17:19 2024
Terminated at Tue Oct 22 11:19:36 2024
Results reported at Tue Oct 22 11:19:36 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   206.54 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   137 sec.
    Turnaround time :                            407 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n08>
Subject: Job 850238: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c207n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:17:16 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:17:16 2024
Terminated at Tue Oct 22 11:19:38 2024
Results reported at Tue Oct 22 11:19:38 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   220.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   149 sec.
    Turnaround time :                            409 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.36±0.01	 r2: 0.01±0.0
RRU Dimer
Filename: (ECFP4.binary.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Dimer/(ECFP4.binary.1024)_NGB_scores.json


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.35±0.0	 r2: 0.01±0.0
RRU Trimer
Filename: (ECFP4.binary.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Trimer/(ECFP4.binary.1024)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n14>
Subject: Job 850240: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c207n14>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:18:10 2024
                            <4*c207n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:18:10 2024
Terminated at Tue Oct 22 11:20:37 2024
Results reported at Tue Oct 22 11:20:37 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   226.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   147 sec.
    Turnaround time :                            468 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n10>
Subject: Job 850241: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c202n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:18:13 2024
                            <4*c202n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:18:13 2024
Terminated at Tue Oct 22 11:20:39 2024
Results reported at Tue Oct 22 11:20:39 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   232.21 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   146 sec.
    Turnaround time :                            470 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.37±0.0	 r2: 0.01±0.0
Monomer
Filename: (ECFP4.count.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(ECFP4.count.1024)_NGB_scores.json


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
Dimer
Filename: (ECFP4.count.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Dimer/(ECFP4.count.1024)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 850242: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:19:36 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:19:36 2024
Terminated at Tue Oct 22 11:22:04 2024
Results reported at Tue Oct 22 11:22:04 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   228.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   150 sec.
    Turnaround time :                            555 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n05>
Subject: Job 850243: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c207n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:19:36 2024
                            <4*c207n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:19:36 2024
Terminated at Tue Oct 22 11:22:04 2024
Results reported at Tue Oct 22 11:22:04 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   226.53 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   168 sec.
    Turnaround time :                            555 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
Trimer
Filename: (ECFP4.count.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Trimer/(ECFP4.count.1024)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 850244: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:19:36 2024
                            <4*c200n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:19:36 2024
Terminated at Tue Oct 22 11:22:25 2024
Results reported at Tue Oct 22 11:22:25 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   262.38 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.62 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   169 sec.
    Turnaround time :                            576 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.36±0.01	 r2: 0.01±0.0
RRU Monomer
Filename: (ECFP4.count.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Monomer/(ECFP4.count.1024)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n01>
Subject: Job 850245: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:49 2024
Job was executed on host(s) <4*c207n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:20:39 2024
                            <4*c207n08>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:20:39 2024
Terminated at Tue Oct 22 11:23:03 2024
Results reported at Tue Oct 22 11:23:03 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   220.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   146 sec.
    Turnaround time :                            614 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.36±0.0	 r2: 0.01±0.0
RRU Dimer
Filename: (ECFP4.count.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Dimer/(ECFP4.count.1024)_NGB_scores.json
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n09>
Subject: Job 850246: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c202n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:20:40 2024
                            <4*c202n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:20:40 2024
Terminated at Tue Oct 22 11:23:10 2024
Results reported at Tue Oct 22 11:23:10 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   238.01 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   151 sec.
    Turnaround time :                            620 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.36±0.01	 r2: 0.01±0.0
RRU Trimer
Filename: (ECFP4.count.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Trimer/(ECFP4.count.1024)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n04>
Subject: Job 850247: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c039n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:21:30 2024
                            <4*c040n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:21:30 2024
Terminated at Tue Oct 22 11:23:43 2024
Results reported at Tue Oct 22 11:23:43 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector count --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   211.60 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   143 sec.
    Turnaround time :                            653 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2048)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
Trimer
Filename: (ECFP5.binary.2048)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Trimer/(ECFP5.binary.2048)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n09>
Subject: Job 850250: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c207n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:22:25 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:22:25 2024
Terminated at Tue Oct 22 11:25:02 2024
Results reported at Tue Oct 22 11:25:02 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   240.46 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.62 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   161 sec.
    Turnaround time :                            732 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2048)}
Average scores:	 r: 0.37±0.01	 r2: 0.01±0.0
RRU Monomer
Filename: (ECFP5.binary.2048)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Monomer/(ECFP5.binary.2048)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n12>
Subject: Job 850251: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c200n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:22:25 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:22:25 2024
Terminated at Tue Oct 22 11:25:06 2024
Results reported at Tue Oct 22 11:25:06 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   246.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.62 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   161 sec.
    Turnaround time :                            736 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2048)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
Monomer
Filename: (ECFP5.binary.2048)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(ECFP5.binary.2048)_NGB_scores.json
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 850248: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:22:25 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:22:25 2024
Terminated at Tue Oct 22 11:25:13 2024
Results reported at Tue Oct 22 11:25:13 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   257.25 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.62 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   168 sec.
    Turnaround time :                            743 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2048)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
Dimer
Filename: (ECFP5.binary.2048)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Dimer/(ECFP5.binary.2048)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n14>
Subject: Job 850249: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c207n14>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:22:25 2024
                            <4*c207n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:22:25 2024
Terminated at Tue Oct 22 11:25:34 2024
Results reported at Tue Oct 22 11:25:34 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   289.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.67 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   207 sec.
    Turnaround time :                            764 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2048)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
RRU Dimer
Filename: (ECFP5.binary.2048)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Dimer/(ECFP5.binary.2048)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n08>
Subject: Job 850252: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c207n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:23:04 2024
                            <4*c207n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:23:04 2024
Terminated at Tue Oct 22 11:25:45 2024
Results reported at Tue Oct 22 11:25:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   258.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.62 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   161 sec.
    Turnaround time :                            775 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2048)}
Average scores:	 r: 0.37±0.0	 r2: 0.01±0.0
RRU Trimer
Filename: (ECFP5.binary.2048)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Trimer/(ECFP5.binary.2048)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n10>
Subject: Job 850253: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c202n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:23:11 2024
                            <4*c202n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:23:11 2024
Terminated at Tue Oct 22 11:26:06 2024
Results reported at Tue Oct 22 11:26:06 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   278.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.62 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   176 sec.
    Turnaround time :                            796 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2048)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
Monomer
Filename: (ECFP5.count.2048)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(ECFP5.count.2048)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c040n04>
Subject: Job 850254: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c040n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:24:05 2024
                            <4*c038n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:24:05 2024
Terminated at Tue Oct 22 11:26:46 2024
Results reported at Tue Oct 22 11:26:46 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   254.03 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.75 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   183 sec.
    Turnaround time :                            836 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2048)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
Dimer
Filename: (ECFP5.count.2048)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Dimer/(ECFP5.count.2048)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 850255: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:25:06 2024
                            <4*c200n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:25:06 2024
Terminated at Tue Oct 22 11:28:14 2024
Results reported at Tue Oct 22 11:28:14 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   299.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.67 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   188 sec.
    Turnaround time :                            924 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2048)}
Average scores:	 r: 0.37±0.01	 r2: 0.01±0.0
RRU Monomer
Filename: (ECFP5.count.2048)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Monomer/(ECFP5.count.2048)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n11>
Subject: Job 850257: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c207n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:25:47 2024
                            <4*c207n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:25:47 2024
Terminated at Tue Oct 22 11:28:39 2024
Results reported at Tue Oct 22 11:28:39 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   272.34 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.62 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   185 sec.
    Turnaround time :                            949 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2048)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
RRU Trimer
Filename: (ECFP5.count.2048)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Trimer/(ECFP5.count.2048)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n01>
Subject: Job 850259: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c207n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:25:47 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:25:47 2024
Terminated at Tue Oct 22 11:28:46 2024
Results reported at Tue Oct 22 11:28:46 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   272.47 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.62 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   181 sec.
    Turnaround time :                            956 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2048)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
Trimer
Filename: (ECFP5.count.2048)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Trimer/(ECFP5.count.2048)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 850256: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:25:47 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:25:47 2024
Terminated at Tue Oct 22 11:28:54 2024
Results reported at Tue Oct 22 11:28:54 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   290.12 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.67 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   191 sec.
    Turnaround time :                            964 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2048)}
Average scores:	 r: 0.37±0.0	 r2: 0.01±0.0
RRU Dimer
Filename: (ECFP5.count.2048)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Dimer/(ECFP5.count.2048)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n14>
Subject: Job 850258: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c207n14>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:25:47 2024
                            <4*c207n08>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:25:47 2024
Terminated at Tue Oct 22 11:28:57 2024
Results reported at Tue Oct 22 11:28:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   302.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.89 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   203 sec.
    Turnaround time :                            967 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 4096)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
Monomer
Filename: (ECFP6.binary.4096)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(ECFP6.binary.4096)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n10>
Subject: Job 850260: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:50 2024
Job was executed on host(s) <4*c202n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:26:06 2024
                            <4*c202n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:26:06 2024
Terminated at Tue Oct 22 11:29:52 2024
Results reported at Tue Oct 22 11:29:52 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   358.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.90 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   226 sec.
    Turnaround time :                            1022 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 4096)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
Dimer
Filename: (ECFP6.binary.4096)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Dimer/(ECFP6.binary.4096)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n12>
Subject: Job 850261: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:51 2024
Job was executed on host(s) <4*c200n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:28:15 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:28:15 2024
Terminated at Tue Oct 22 11:32:00 2024
Results reported at Tue Oct 22 11:32:00 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   349.28 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.20 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   226 sec.
    Turnaround time :                            1149 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 4096)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
RRU Monomer
Filename: (ECFP6.binary.4096)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Monomer/(ECFP6.binary.4096)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n09>
Subject: Job 850263: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:51 2024
Job was executed on host(s) <4*c207n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:28:56 2024
                            <4*c207n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:28:56 2024
Terminated at Tue Oct 22 11:32:06 2024
Results reported at Tue Oct 22 11:32:06 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   286.05 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.22 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   207 sec.
    Turnaround time :                            1156 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 4096)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
RRU Dimer
Filename: (ECFP6.binary.4096)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Dimer/(ECFP6.binary.4096)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n01>
Subject: Job 850264: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:51 2024
Job was executed on host(s) <4*c207n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:28:56 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:28:56 2024
Terminated at Tue Oct 22 11:32:43 2024
Results reported at Tue Oct 22 11:32:43 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   336.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.20 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   237 sec.
    Turnaround time :                            1192 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 4096)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
RRU Trimer
Filename: (ECFP6.binary.4096)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/RRU Trimer/(ECFP6.binary.4096)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n08>
Subject: Job 850265: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:51 2024
Job was executed on host(s) <4*c207n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:28:59 2024
                            <4*c207n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:28:59 2024
Terminated at Tue Oct 22 11:32:51 2024
Results reported at Tue Oct 22 11:32:51 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   342.39 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.20 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   234 sec.
    Turnaround time :                            1200 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 4096)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
Trimer
Filename: (ECFP6.binary.4096)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Trimer/(ECFP6.binary.4096)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 850262: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:51 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:28:56 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:28:56 2024
Terminated at Tue Oct 22 11:33:00 2024
Results reported at Tue Oct 22 11:33:00 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   369.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.00 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   245 sec.
    Turnaround time :                            1209 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 1.6994636371262764e-05), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 4096)}
Average scores:	 r: 0.38±0.0	 r2: 0.01±0.0
Monomer
Filename: (ECFP6.count.4096)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/test/Monomer/(ECFP6.count.4096)_NGB_scores.json

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n09>
Subject: Job 850266: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:51 2024
Job was executed on host(s) <4*c202n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:29:54 2024
                            <4*c202n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:29:54 2024
Terminated at Tue Oct 22 11:34:18 2024
Results reported at Tue Oct 22 11:34:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   387.11 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.27 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   266 sec.
    Turnaround time :                            1287 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n10>
Subject: Job 850267: <ecfp_radius_tructure_only> in cluster <Hazel> Exited

Job <ecfp_radius_tructure_only> was submitted from host <c202n10> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:12:51 2024
Job was executed on host(s) <4*c202n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:34:19 2024
                            <4*c202n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 11:34:19 2024
Terminated at Tue Oct 22 11:36:54 2024
Results reported at Tue Oct 22 11:36:54 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   125.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.62 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   156 sec.
    Turnaround time :                            1443 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1370), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007905906720869482), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1495), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006430138014679556), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007472821088173099), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1733), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008963043580120022), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1400), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1148), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 952), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008760572843309506), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1566), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008045551797779908), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 963), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1586), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007037051785522883), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009068294654952778), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1375), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1492), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008414210640493635), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1704), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006426091199408828), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000830836314626077), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1398), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000836657944762848), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008438899038925204), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1281), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1084), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008940566393880129), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1696), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1391), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007901161349175408), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1661), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1734), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008691440432984727), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008200786451311866), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1761), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008369922539126443), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008543180861774535), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008986415355022352), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1938), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1348), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000674829021899926), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1756), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008556398514286285), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1310), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006209229253522192), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1646), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1436), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1932), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1158), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007677978077717768), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1729), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008629666674284911), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008986278167359207), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1880), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1029), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007960704796237633), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1721), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005554101034136581), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005402097040178369), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008680624664960327), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1785), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1713), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008799321049584008), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1266), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1269), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006718097199856774), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008605796508432643), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1140), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008646133003449553), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008139089941773593), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000736711936306894), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1674), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1432), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008960045457463048), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1392), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1530), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005966934654029212), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000866592518601751), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007297841595621114), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1186), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000709036196494522), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007623409718010604), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1555), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1160), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007330869794618384), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1636), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1166), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008226201494086749), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1660), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1404), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1642), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007513518444754539), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008395190209137442), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000996333440502042), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1962), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008224058793325117), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1822), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1706), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008152671754552167), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1580), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008562096396463376), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1668), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1646), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008896002953981084), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1527), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1608), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008931482600161995), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1814), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007358077849158379), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1504), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1425), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008484543101743504), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008667082378557275), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1776), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000861187156631216), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1816), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000843036801571963), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1688), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1742), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1727), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1815), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007097649390180827), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1791), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1755), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1842), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008736300513914808), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007760014814227733), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009939905064552234), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1967), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1508), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1192), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008386478306994112), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1627), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1307), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000990325923417942), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1936), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009293469734774268), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009060492393256991), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008229453814020589), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1326), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000999259837610302), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1883), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1570), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1525), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008736056249181534), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1747), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1293), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009998527496034577), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1973), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.42±0.19	 r2: 0.16±0.18
RRU Monomer
Filename: (ECFP3.count.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP3.count.512)_NGB_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP3.count.512)_NGB_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP3.count.512)_NGB_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c017n02>
Subject: Job 850417: <ecfp_radius_tructure_only> in cluster <Hazel> Done

Job <ecfp_radius_tructure_only> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:46:17 2024
Job was executed on host(s) <4*c017n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 12:26:32 2024
                            <4*c019n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 12:26:32 2024
Terminated at Tue Oct 22 18:01:12 2024
Results reported at Tue Oct 22 18:01:12 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   91835.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.99 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   20082 sec.
    Turnaround time :                            22495 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008834761852501969), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1527), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000855046451054341), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007707889638772429), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008377664459878197), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1832), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008474538932195536), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1390), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1600), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008208063726843226), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008321841536074523), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007681644437993824), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1525), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000979822883652993), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1996), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000865964056571955), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1551), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006300422315393817), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008580521307663695), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000781706998892242), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1581), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006727401013355605), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009508433292822249), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007761803980932377), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1990), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1170), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1632), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1714), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000798837959861518), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008584726856422139), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1592), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1295), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006575001260429064), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006659884605124), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009985648661548572), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1968), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.42±0.19	 r2: 0.16±0.17
Trimer
Filename: (ECFP3.count.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP3.count.512)_NGB_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP3.count.512)_NGB_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP3.count.512)_NGB_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c022n03>
Subject: Job 850416: <ecfp_radius_tructure_only> in cluster <Hazel> Done

Job <ecfp_radius_tructure_only> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:46:17 2024
Job was executed on host(s) <4*c022n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 12:26:32 2024
                            <4*c023n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 12:26:32 2024
Terminated at Tue Oct 22 18:06:30 2024
Results reported at Tue Oct 22 18:06:30 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   94148.34 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.99 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   20401 sec.
    Turnaround time :                            22813 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008541554117625743), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1738), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007164857374379684), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1596), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1638), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009462948625118099), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1978), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1732), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000980478621832726), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1971), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1823), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008318726834666176), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1526), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008301985460204643), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1377), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1682), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009913954215551412), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1935), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008214926327586936), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1625), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008550335069324814), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008539652704374437), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008840513978580945), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1814), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008702336502333536), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1649), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1692), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007352340501510327), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009371505910303816), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009305319930117087), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1970), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1745), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1256), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1707), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008431172119257572), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1550), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007746164525346237), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1329), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006117524393129744), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1784), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.42±0.19	 r2: 0.16±0.18
RRU Dimer
Filename: (ECFP3.binary.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.binary.512)_NGB_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.binary.512)_NGB_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.binary.512)_NGB_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c037n02>
Subject: Job 850412: <ecfp_radius_tructure_only> in cluster <Hazel> Done

Job <ecfp_radius_tructure_only> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:46:16 2024
Job was executed on host(s) <4*c037n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 12:03:27 2024
                            <4*c030n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 12:03:27 2024
Terminated at Tue Oct 22 18:23:51 2024
Results reported at Tue Oct 22 18:23:51 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   101225.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.98 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   22833 sec.
    Turnaround time :                            23855 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000903104108502095), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1721), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1646), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000974905398983935), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1562), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1813), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009087620430246233), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1794), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009953738409597774), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1394), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008500584050689122), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1786), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1750), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008367936307079647), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1764), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1510), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000979822883652993), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1996), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007974629130300198), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008489743912715144), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008873772101696443), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1616), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000855848855541274), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1358), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008590104493359278), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000734062625256347), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008715858185876838), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008302969130513164), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1998), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1280), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1350), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008507616980986167), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008767415618191528), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1652), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007860769169258182), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1707), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1465), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009985648661548572), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1968), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.42±0.19	 r2: 0.16±0.18
RRU Dimer
Filename: (ECFP4.binary.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP4.binary.1024)_NGB_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP4.binary.1024)_NGB_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP4.binary.1024)_NGB_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c006n02>
Subject: Job 850424: <ecfp_radius_tructure_only> in cluster <Hazel> Done

Job <ecfp_radius_tructure_only> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:46:17 2024
Job was executed on host(s) <4*c006n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 12:28:18 2024
                            <4*c010n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 12:28:18 2024
Terminated at Tue Oct 22 18:29:19 2024
Results reported at Tue Oct 22 18:29:19 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   99706.14 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.82 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               60.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   21661 sec.
    Turnaround time :                            24182 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1838), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1182), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008864905557445618), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1790), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007768792433578894), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008473947169248349), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1994), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1669), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1784), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007217132115697834), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1732), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009055794383428788), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1748), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1656), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008061773510302606), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009917786863883529), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1980), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009917786863883529), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1980), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1606), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1558), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009791807557931564), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1939), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008606550034491965), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000842431293057169), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1744), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1338), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008541355388472277), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008292651655165455), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006483156444273512), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1620), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007951908326598439), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1670), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008849781980573686), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007597740445567468), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006121375402397931), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008769911252206312), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1470), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009985648661548572), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1968), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.42±0.19	 r2: 0.16±0.18
RRU Dimer
Filename: (ECFP3.count.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.count.512)_NGB_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.count.512)_NGB_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.count.512)_NGB_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c022n02>
Subject: Job 850418: <ecfp_radius_tructure_only> in cluster <Hazel> Done

Job <ecfp_radius_tructure_only> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:46:17 2024
Job was executed on host(s) <4*c022n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 12:26:32 2024
                            <4*c022n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 12:26:32 2024
Terminated at Tue Oct 22 18:36:58 2024
Results reported at Tue Oct 22 18:36:58 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   98121.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.98 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   22228 sec.
    Turnaround time :                            24641 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007333269288864369), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1967), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000997497011743372), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1965), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009007912357238915), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1394), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008898248713069442), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1814), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1696), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1489), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007242580378784649), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008347265235269255), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008596986241848659), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009853675508610495), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1998), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008880524935607091), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008824275097112256), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008196495623843332), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1606), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1657), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008106881214816642), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1282), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008894524271833691), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1648), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1099), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007756447817187842), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00099384599105575), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1578), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008674642930621195), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1581), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1447), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1307), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 512)}
Average scores:	 r: 0.42±0.19	 r2: 0.16±0.17
Monomer
Filename: (ECFP3.count.512)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP3.count.512)_NGB_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP3.count.512)_NGB_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP3.count.512)_NGB_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c035n04>
Subject: Job 850414: <ecfp_radius_tructure_only> in cluster <Hazel> Done

Job <ecfp_radius_tructure_only> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:46:17 2024
Job was executed on host(s) <4*c035n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 12:03:27 2024
                            <4*c029n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 12:03:27 2024
Terminated at Tue Oct 22 18:37:55 2024
Results reported at Tue Oct 22 18:37:55 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 3 --vector count --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   105276.23 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.98 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   23690 sec.
    Turnaround time :                            24698 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1444), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008469618676671098), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1761), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1464), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007621730838256868), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009980346842269215), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1929), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1234), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1452), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008825214223821512), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008971726578367252), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000985944433398688), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1959), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009890264939606977), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1928), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1518), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006836449563777606), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1683), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008984128734902429), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1640), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008794519207529969), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1812), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007395812743718762), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009723534040083587), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1991), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008990291868575219), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000871046932876693), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007543115384670322), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1655), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1772), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1547), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008410518372510271), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009578804078132447), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1882), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1628), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1547), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006377244207265868), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007976735833237472), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.42±0.19	 r2: 0.16±0.17
Monomer
Filename: (ECFP4.binary.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP4.binary.1024)_NGB_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP4.binary.1024)_NGB_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP4.binary.1024)_NGB_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c013n01>
Subject: Job 850420: <ecfp_radius_tructure_only> in cluster <Hazel> Done

Job <ecfp_radius_tructure_only> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:46:17 2024
Job was executed on host(s) <4*c013n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 12:28:18 2024
                            <4*c012n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 12:28:18 2024
Terminated at Tue Oct 22 18:39:04 2024
Results reported at Tue Oct 22 18:39:04 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "Monomer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   102795.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.69 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               60.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   22246 sec.
    Turnaround time :                            24767 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008358658049101924), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1509), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00047501640256697876), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008187990081252073), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007629158091321403), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008644226295401092), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007602022997790023), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1767), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1417), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008170868442274324), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1642), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1686), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008696992869600073), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1828), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007878382101988475), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1700), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008630210452919453), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008674866808183868), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1441), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008379295148482233), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007463955228549879), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1761), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1804), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1734), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1627), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008843480476070125), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005811946959898985), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000852500811652045), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007851977319250378), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008215970873562214), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1703), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008543103722121311), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008914894489781155), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1482), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1300), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1215), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 1024)}
Average scores:	 r: 0.42±0.19	 r2: 0.16±0.17
Dimer
Filename: (ECFP4.binary.1024)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP4.binary.1024)_NGB_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP4.binary.1024)_NGB_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP4.binary.1024)_NGB_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c008n04>
Subject: Job 850421: <ecfp_radius_tructure_only> in cluster <Hazel> Done

Job <ecfp_radius_tructure_only> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 11:46:17 2024
Job was executed on host(s) <4*c008n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 12:28:18 2024
                            <4*c010n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Oct 22 12:28:18 2024
Terminated at Tue Oct 22 18:44:52 2024
Results reported at Tue Oct 22 18:44:52 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius_tructure_only" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type NGB --radius 4 --vector binary --target "Rg1 (nm)" --oligo_type "Dimer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   104252.24 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.80 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               60.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   22613 sec.
    Turnaround time :                            25115 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_ecfp_NGB.err> for stderr output of this job.

